{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.1.3-cp311-cp311-macosx_14_0_x86_64.whl (6.9 MB)\n",
      "Requirement already satisfied: openai in ./.venv/lib/python3.11/site-packages (1.56.2)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: whisper in ./.venv/lib/python3.11/site-packages (1.1.10)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from openai) (0.28.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.11/site-packages (from openai) (0.8.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.11/site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.11/site-packages (from whisper) (1.17.0)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.8/447.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-macosx_10_9_x86_64.whl (184 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp311-cp311-macosx_10_9_x86_64.whl (287 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.7/287.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_10_12_x86_64.whl (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.1\n",
      "  Using cached safetensors-0.4.5-cp311-cp311-macosx_10_12_x86_64.whl (392 kB)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.0-cp311-cp311-macosx_10_9_x86_64.whl (124 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: pydub, urllib3, safetensors, regex, pyyaml, numpy, fsspec, filelock, charset-normalizer, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed charset-normalizer-3.4.0 filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.26.5 numpy-2.1.3 pydub-0.25.1 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.4.5 tokenizers-0.21.0 transformers-4.47.0 urllib3-2.2.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy openai pydub whisper transformers\n",
    "%pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Real-Time Red Sales Agent\n",
    "# A notebook to demonstrate real-time transcription with Whisper and actionable insights using GPT.\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import openai\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "from openai import OpenAI\n",
    "import tempfile\n",
    "import threading\n",
    "import queue\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up API keys and environment variables\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define constants\n",
    "TEMP_DIR = tempfile.mkdtemp()\n",
    "CHUNK_DURATION = 3  # Duration of audio chunks in seconds\n",
    "WHISPER_MODEL_SIZE = \"small\"  # Options: tiny, small, medium, large\n",
    "GPT_MODEL = \"gpt-4\"\n",
    "\n",
    "# Initialize the Whisper model\n",
    "client = OpenAI()\n",
    "\n",
    "# Queue for audio chunks\n",
    "audio_queue = queue.Queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define audio streaming and recording logic\n",
    "def record_stream_to_file(stream, output_dir=TEMP_DIR):\n",
    "    \"\"\"\n",
    "    Record stream audio to files as .mp3 in CHUNK_DURATION second chunks.\n",
    "    Args:\n",
    "        stream: Audio input stream (mocked as example)\n",
    "        output_dir (str): Directory to save audio chunks\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Starting audio recording...\")\n",
    "    for i, chunk in enumerate(stream):\n",
    "        file_path = os.path.join(output_dir, f\"chunk_{i}.mp3\")\n",
    "        chunk.export(file_path, format=\"mp3\")\n",
    "        print(f\"[INFO] Saved chunk: {file_path}\")\n",
    "        audio_queue.put(file_path)\n",
    "        time.sleep(CHUNK_DURATION)  # Simulate real-time streaming delay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transcription logic\n",
    "def transcribe_file(model_str, file_path):\n",
    "    \"\"\"\n",
    "    Transcribe an audio file using Whisper.\n",
    "    Args:\n",
    "        model: Whisper model instance\n",
    "        file_path (str): Path to the audio file\n",
    "    Returns:\n",
    "        str: Transcription text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "            model=model_str, \n",
    "            file=file_path\n",
    "        )\n",
    "        print(f\"[TRANSCRIPTION] {transcription.text}\")\n",
    "        return transcription.text\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Transcription failed for {file_path}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GPT streaming logic\n",
    "def gpt_streaming_logic(transcription):\n",
    "    \"\"\"\n",
    "    Stream transcription to GPT and print insights in real-time.\n",
    "    Args:\n",
    "        transcription (str): Input transcription text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            engine=GPT_MODEL,\n",
    "            prompt=f\"Provide actionable sales insights for the following transcript:\\n{transcription}\",\n",
    "            max_tokens=150,\n",
    "            stream=True,\n",
    "        )\n",
    "        print(\"[GPT INSIGHTS]\")\n",
    "        for chunk in response:\n",
    "            if \"choices\" in chunk and \"text\" in chunk[\"choices\"][0]:\n",
    "                print(chunk[\"choices\"][0][\"text\"], end=\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] GPT streaming failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mocking an audio stream (replace with actual stream in production)\n",
    "def generate_mock_stream():\n",
    "    \"\"\"\n",
    "    Generate a mock audio stream using pydub.\n",
    "    Returns:\n",
    "        list: Simulated audio chunks as AudioSegment objects\n",
    "    \"\"\"\n",
    "    audio = AudioSegment.from_file(\"example_audio.mp3\", format=\"mp3\")\n",
    "    chunks = [audio[i * 1000 * CHUNK_DURATION:(i + 1) * 1000 * CHUNK_DURATION] for i in range(len(audio) // (1000 * CHUNK_DURATION))]\n",
    "    return chunks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threaded transcription and GPT logic\n",
    "def transcription_and_gpt_pipeline():\n",
    "    \"\"\"\n",
    "    Process audio chunks from the queue for transcription and GPT response.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        if not audio_queue.empty():\n",
    "            file_path = audio_queue.get()\n",
    "            transcription = transcribe_file('whisper-1', file_path)\n",
    "            if transcription:\n",
    "                gpt_streaming_logic(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Start transcription and GPT pipeline in a separate thread\n",
    "    threading.Thread(target=transcription_and_gpt_pipeline, daemon=True).start()\n",
    "    \n",
    "    # Simulate real-time audio recording and streaming\n",
    "    audio_stream = generate_mock_stream()  # Replace with actual stream in production\n",
    "    record_stream_to_file(audio_stream)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
