{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Transcription and GPT Response using Whisper and OpenAI\n",
    "\n",
    "### Why is this worth talking about?\n",
    "\n",
    "In order for a sales agent to be effective it needs to provide fast, real-time feedback while the conversation is ongoing. In order for this to happen we need to have an open stream of audio input so it can be processed in real-time.\n",
    "\n",
    "The project is written in python because we get the best goodies out of the box.  We will be able to expand on this example with additional AI models and audio formatting libraries programmatically.  Though similar options are available through node.js, they are not as easy to use.\n",
    "\n",
    "We are choosing to use the OpenAI whisper local model so we can pass in the streaming bytes explicitly.  Using the new version we would have to slice up the audio so it can process mp3s.  This is not real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up OpenAI API Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=\"YOUR_API_KEY_HERE\"  \n",
    ")\n",
    "\n",
    "gpt_queue = Queue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for GPT queue processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def process_gpt_queue():\n",
    "    \"\"\"\n",
    "    Processes lines from the transcription queue with GPT to generate actionable insights.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            if not gpt_queue.empty():\n",
    "                line = gpt_queue.get()\n",
    "                stream = client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"\"\"\n",
    "                            You are an expert sales coaching assistant...\n",
    "                            \"\"\".strip()},\n",
    "                        {\"role\": \"user\", \"content\": f\"Evaluate this line: {line}\"}\n",
    "                    ],\n",
    "                    stream=True,\n",
    "                )\n",
    "                for chunk in stream:\n",
    "                    if chunk.choices[0].delta.content:\n",
    "                        print(chunk.choices[0].delta.content, end=\"\")\n",
    "            else:\n",
    "                sleep(0.25)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in GPT processing: {e}\")\n",
    "\n",
    "### Existing Code\n",
    "\n",
    "gpt_thread = Thread(target=process_gpt_queue, daemon=True)\n",
    "gpt_thread.start()\n",
    "# Cue the user that we're ready to go.\n",
    "print(\"Model loaded * You can start your call now\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we are putting this in a thread is because we want to be able to process the GPT responses in real time.\n",
    "\n",
    "The audio indput and the real-time connection to the source microphone will be taking a thread so we need a new thread to do the inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the audio input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# The last time a recording was retrieved from the queue.\n",
    "phrase_time = None\n",
    "\n",
    "# Thread safe Queue for passing data from the threaded recording callback.\n",
    "data_queue = Queue()\n",
    "\n",
    "# We use SpeechRecognizer to record our audio because it has a nice feature where it can detect when speech ends.\n",
    "recorder = sr.Recognizer()\n",
    "recorder.energy_threshold = args.energy_threshold\n",
    "\n",
    "# Definitely do this, dynamic energy compensation lowers the energy threshold dramatically to a point where the SpeechRecognizer never stops recording.\n",
    "recorder.dynamic_energy_threshold = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a local store for the audio that we are receiving so it can be processed in meaningful chunks by the GPT.  We can do this with a local file store but we chose to do it with a queue.  Basically each line spoken is added to the queue and when our phrase_time is reached (say 2 seconds) we send the line to the GPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually processing the audio bytes and generating the transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get our model\n",
    "audio_model = whisper.load_model(model)\n",
    "\n",
    "# Combine audio data from queue\n",
    "audio_data = b''.join(data_queue.queue)\n",
    "data_queue.queue.clear()\n",
    "\n",
    "# Convert in-ram buffer to something the model can use directly without needing a temp file.\n",
    "# Convert data from 16 bit wide integers to floating point with a width of 32 bits.\n",
    "# Clamp the audio stream frequency to a PCM wavelength compatible default of 32768hz max.\n",
    "audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "\n",
    "# Read the transcription.\n",
    "result = audio_model.transcribe(audio_np, fp16=torch.cuda.is_available())\n",
    "text = result['text'].strip()\n",
    "\n",
    "# If we detected a pause between recordings, add a new item to our transcription.\n",
    "# Otherwise edit the existing one.\n",
    "if phrase_complete:\n",
    "  transcription.append(text)\n",
    "  gpt_queue.put(text)\n",
    "else:\n",
    "  transcription[-1] = text\n",
    "  gpt_queue.put(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code processes audio input in real-time using OpenAI's Whisper model. It captures audio through a microphone, converts the raw bytes to a numpy array with proper audio formatting, transcribes it to text, and adds each transcribed segment to both a transcription list and a GPT queue for further analysis. The code handles continuous streaming by detecting pauses between phrases and either appending new text or updating the existing transcription.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Whisper Implementations\n",
    "\n",
    "| Feature | OpenAI Whisper (Local) | OpenAI API Whisper |\n",
    "|---------|------------------------|-------------------|\n",
    "| Setup | Requires local installation and model download | API key only |\n",
    "| Processing | Local CPU/GPU processing | Cloud-based processing |\n",
    "| Model Options | Multiple model sizes (tiny to large) | Single optimized model |\n",
    "| Cost | Free (after download) | Pay per minute of audio |\n",
    "| Latency | Depends on local hardware | Network-dependent |\n",
    "| Integration | More code required for audio handling | Simple API calls |\n",
    "| Customization | Full control over parameters | Limited configuration |\n",
    "| Dependencies | Requires torch, numpy, etc. | Minimal dependencies |\n",
    "| Streaming | Manual implementation needed | Built-in streaming support |\n",
    "| Resource Usage | Uses local system resources | Cloud resources |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges of Real-Time Streaming Integration\n",
    "\n",
    "When building real-time streaming applications, several key challenges need to be addressed:\n",
    "\n",
    "1. **Latency Management**: Balancing between buffering enough audio data for accurate transcription while maintaining real-time responsiveness\n",
    "\n",
    "2. **Resource Usage**: Managing CPU and memory consumption, especially when processing continuous audio streams\n",
    "\n",
    "3. **Error Handling**: Gracefully handling network issues, audio device problems, and service interruptions\n",
    "\n",
    "4. **Queue Management**: Coordinating multiple queues for audio data, transcriptions, and API responses without bottlenecks\n",
    "\n",
    "5. **State Management**: Tracking the state of ongoing streams and managing transitions between phrases\n",
    "\n",
    "These challenges require careful consideration of buffer sizes, timeout values, and error recovery strategies to create a smooth user experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
